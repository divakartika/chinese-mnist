)
history <- model2 %>%
fit(x = train_x,
y = train_y,
epoch = 15,
validation_data = list(val_x, val_y),
batch_size = 500,
# verbose = 1
)
model2 <- keras_model_sequential() %>%
# Convolutional layer
layer_conv_2d(input_shape = c(64,64,1),
filters = 16,
kernel_size = c(3,3), # 3 x 3 filters
padding = "same",
activation = "relu") %>%
# Max pooling layer
layer_max_pooling_2d(pool_size = c(2,2)) %>%
# Flattening layer
layer_flatten() %>%
# Dense layer
layer_dense(units = 16,
activation = "relu") %>%
# output layer
layer_dense(units = num_class, # jumlah kelas target
activation = "softmax", # untuk multikelas
name = "ouput")
model2 %>%
compile(loss = loss_categorical_crossentropy(),
optimizer = optimizer_adam(learning_rate = 0.01),
metrics = "accuracy")
history <- model2 %>%
fit(x = train_x,
y = train_y,
epoch = 30,
validation_data = list(val_x, val_y),
batch_size = 500,
# verbose = 1
)
gc()
chinese_meta <- read.csv("data-input/chinese_mnist.csv")
source("I:/My Drive/Algoritma/projects/lbb-nn/fixed_class.R", echo=TRUE)
library(dplyr)
chinese_mnist <- read.csv("data-input/chineseMNIST.csv")
chinese_meta <- read.csv("data-input/chinese_mnist.csv")
chinese_mnist$label <- chinese_meta$code
chinese_mnist$value <- chinese_meta$value
chinese_mnist <- chinese_mnist %>%
mutate(label = ifelse(label > 0, label-1, label))
#
# test_mnist <- test_mnist %>%
#   mutate(label = ifelse(label > 9, label-1, label))
set.seed(100)
index <- initial_split(data = chinese_mnist, prop = 9/10, strata = "label")
library(rsample)
set.seed(100)
index <- initial_split(data = chinese_mnist, prop = 9/10, strata = "label")
data_test <- testing(index)
data_train <- training(index)
index <- initial_split(data = data_train, prop = 8/9, strata = "label")
data_val <- testing(index)
data_train <- training(index)
length(data_train)
table(data_test$label)
table(chinese_mnist$label)
set.seed(100)
index <- initial_split(data = chinese_mnist, prop = 9/10, strata = "label")
data_test <- testing(index)
data_train <- training(index)
table(data_test$label)
set.seed(100)
index <- initial_split(data = chinese_mnist, prop = 9/10, strata = "character")
data_test <- testing(index)
data_train <- training(index)
table(data_test$label)
set.seed(100)
index <- initial_split(data = chinese_mnist, prop = 9/10, strata = "value")
data_test <- testing(index)
data_train <- training(index)
table(data_test$label)
index <- initial_split(data = chinese_mnist, prop = 9/10, strata = "value")
data_test <- testing(index)
data_train <- training(index)
table(data_test$label)
table(data_train$label)
set.seed(100)
index <- initial_split(data = chinese_mnist, prop = 8/10, strata = "label")
data_test <- testing(index)
data_train <- training(index)
table(data_train$label)
set.seed(100)
index <- initial_split(data = chinese_mnist, prop = 0.8, strata = "label")
data_test <- testing(index)
data_train <- training(index)
table(data_train$label)
index <- createDataPartition(chinese_mnist$label, p = 0.8, list = FALSE)
library(caret)
index <- createDataPartition(chinese_mnist$label, p = 0.8, list = FALSE)
data_train <- chinese_mnist[ index,]
data_test  <- chinese_mnist[-index,]
table(data_train$label)
index <- createDataPartition(chinese_mnist$label, p = 0.5, list = FALSE)
data_train <- chinese_mnist[ index,]
data_test  <- chinese_mnist[-index,]
table(data_train$label)
chinese_mnist <- read.csv("data-input/chineseMNIST.csv")
chinese_meta <- read.csv("data-input/chinese_mnist.csv")
chinese_mnist$label <- chinese_meta$code
chinese_mnist$value <- chinese_meta$value
chinese_mnist <- chinese_mnist %>%
mutate(label = ifelse(label > 0, label-1, label))
#
tindex <- createDataPartition(chinese_mnist$label, p = 0.5, list = FALSE)
data_train <- chinese_mnist[ tindex,]
data_test  <- chinese_mnist[-tindex,]
table(data_train$label)
tindex <- createDataPartition(as.factor(chinese_mnist$label), p = 0.5, list = FALSE)
data_train <- chinese_mnist[ tindex,]
data_test  <- chinese_mnist[-tindex,]
table(data_train$label)
set.seed(100)
train_index <- createDataPartition(as.factor(chinese_mnist$label), p = 0.8, list = FALSE)
data_train <- chinese_mnist[ train_index,]
data_test  <- chinese_mnist[-train_index,]
set.seed(100)
train_index <- createDataPartition(as.factor(data_test$label), p = 0.5, list = FALSE)
data_val <- chinese_mnist[ train_index,]
data_test  <- chinese_mnist[-train_index,]
table(data_train$label)
table(data_test$label)
table(data_test$label)
table(data_val$label)
set.seed(100)
train_index <- createDataPartition(as.factor(chinese_mnist$label), p = 0.8, list = FALSE)
data_train <- chinese_mnist[ train_index,]
data_test  <- chinese_mnist[-train_index,]
set.seed(100)
test_index <- createDataPartition(as.factor(data_test$label), p = 0.5, list = FALSE)
data_val   <- chinese_mnist[ test_index,]
data_test  <- chinese_mnist[-test_index,]
table(data_val$label)
set.seed(100)
train_index <- createDataPartition(as.factor(chinese_mnist$label), p = 0.9, list = FALSE)
data_train <- chinese_mnist[ train_index,]
data_test  <- chinese_mnist[-train_index,]
set.seed(100)
test_index <- createDataPartition(as.factor(data_test$label), p = 8/9, list = FALSE)
data_train <- chinese_mnist[ test_index,]
data_val   <- chinese_mnist[-test_index,]
table(data_val$label)
table(data_train$label)
table(data_testl$label)
table(data_test$label)
set.seed(100)
train_index <- createDataPartition(as.factor(chinese_mnist$label), p = 0.9, list = FALSE)
data_train <- chinese_mnist[ train_index,]
data_test  <- chinese_mnist[-train_index,]
set.seed(100)
test_index <- createDataPartition(as.factor(data_train$label), p = 8/9, list = FALSE)
data_train <- chinese_mnist[ test_index,]
data_val   <- chinese_mnist[-test_index,]
table(data_test$label)
table(data_val$label)
set.seed(100)
train_index <- createDataPartition(as.factor(chinese_mnist$label), p = 0.8, list = FALSE)
data_train <- chinese_mnist[ train_index,]
data_test  <- chinese_mnist[-train_index,]
set.seed(100)
test_index <- createDataPartition(as.factor(data_test$label), p = 0.5, list = FALSE)
data_val   <- data_test[ test_index,]
data_test  <- data_test[-test_index,]
table(data_val$label)
table(data_test$label)
table(data_train$label)
saveRDS(mtcars, file="mtcars.RDS")
readRDS("mtcars.RDS")
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
fig.align = "center"
)
options(scipen = 9999)
chinese_mnist <- read.csv("data-input/chineseMNIST.csv") # main dataset
chinese_meta <- read.csv("data-input/chinese_mnist.csv") # index (contains labels)
# combine the metadata to the main dataset
chinese_mnist$label <- chinese_meta$code
chinese_mnist$value <- chinese_meta$value
# rearrange the dataset so "label", "value", and "character" are the first two columns
chinese_mnist <- chinese_mnist %>%
select(label, value, character, everything())
library(dplyr)
chinese_mnist <- chinese_mnist %>%
select(label, value, character, everything())
saveRDS(chinese_mnist, "data-input/chinese_mnist.RDS")
chinese_mnist <- readRDS("data-input/chinese_mnist.RDS")
chinese_mnist <- chinese_mnist %>%
mutate(label = ifelse(label > 0, label-1, label))
sort(unique(chinese_mnist$label))
set.seed(100)
train_index <- createDataPartition(as.factor(chinese_mnist$label), p = 0.8, list = FALSE)
library(keras)
library(dplyr)
library(caret)
set.seed(100)
train_index <- createDataPartition(as.factor(chinese_mnist$label), p = 0.8, list = FALSE)
data_train <- chinese_mnist[ train_index,]
data_test  <- chinese_mnist[-train_index,]
set.seed(100)
test_index <- createDataPartition(as.factor(data_test$label), p = 0.5, list = FALSE)
data_val   <- data_test[ test_index,]
data_test  <- data_test[-test_index,]
data_train_x <- data_train %>%
select(-c(label,character,value)) %>% # take only the predictors
as.matrix()/255 # change the data type into matrix and do minmax scaling
data_train_y <- data_train$label # take only the labels
data_val_x <- data_val %>%
select(-c(label,character,value)) %>%
as.matrix()/255
data_val_y <- data_val$label
data_test_x <- data_test %>%
select(-c(label,character,value)) %>%
as.matrix()/255
data_test_y <- data_test$label
# Change predictors to arrays
train_x <- array_reshape(data_train_x, dim=dim(data_train_x))
val_x <- array_reshape(data_val_x, dim=dim(data_val_x))
test_x <- array_reshape(data_test_x, dim=dim(data_test_x))
# One-hot encoding target variable
train_y <- to_categorical(data_train_y)
val_y <- to_categorical(data_val_y)
test_y <- to_categorical(data_test_y)
input_dim <- ncol(train_x) # dimension of predictors
num_class <- n_distinct(data_train$label)
model1 <- keras_model_sequential() %>%
# input layer + first hidden layer
layer_dense(input_shape = input_dim, # dimension of predictors
units = 64, # number of neurons/nodes
activation = "relu", # activation function
name = "hidden_1") %>%
# Dense layer
layer_dense(units = 32,
activation = "relu") %>% # to produce non-negative values
# output layer
layer_dense(units = num_class, # num. of target classes
activation = "softmax", # for multiclass classification case
name = "ouput")
model1 %>%
compile(loss = loss_categorical_crossentropy(),
optimizer = optimizer_adam(learning_rate = 0.01),
metrics = "accuracy")
history <- model1 %>%
fit(x = train_x,
y = train_y,
epoch = 15,
validation_data = list(val_x, val_y),
batch_size = 1000)
plot(history)
saveRDS(model1, file = "data-output/model1.RDS")
plot(model1)
model1 <- readRDS("data-output/model1.RDS")
plot(model1)
saveRDS(history, file = "data-output/history1.RDS")
history <- readRDS("data-output/history1.RDS")
plot(history)
model1 <- readRDS("data-output/model1.RDS")
pred <- predict(model1, test_x) %>%
k_argmax() %>% # take the highest probability value
as.array() %>%
as.factor()
pred <- predict(model1, test_x) %>%
k_argmax() %>% # take the highest probability value
as.array() %>%
as.factor()
model1 <- readRDS("data-output/model1.RDS")
history <- readRDS("data-output/history1.RDS")
plot(history)
pred <- predict(model1, test_x) %>%
k_argmax() %>% # take the highest probability value
as.array() %>%
as.factor()
model1 <- readRDS("data-output/model1.RDS")
history <- readRDS("data-output/history1.RDS")
plot(history)
pred <- predict(model1, test_x) %>%
k_argmax() %>% # take the highest probability value
as.array() %>%
as.factor()
pred <- predict(model1, test_x) # %>%
model1 <- keras_model_sequential() %>%
# input layer + first hidden layer
layer_dense(input_shape = input_dim, # dimension of predictors
units = 64, # number of neurons/nodes
activation = "relu", # activation function
name = "hidden_1") %>%
# Dense layer
layer_dense(units = 32,
activation = "relu") %>% # to produce non-negative values
# output layer
layer_dense(units = num_class, # num. of target classes
activation = "softmax", # for multiclass classification case
name = "ouput")
model1 %>%
compile(loss = loss_categorical_crossentropy(),
optimizer = optimizer_adam(learning_rate = 0.01),
metrics = "accuracy")
history <- model1 %>%
fit(x = train_x,
y = train_y,
epoch = 15,
validation_data = list(val_x, val_y),
batch_size = 1000)
plot(history)
pred <- predict(model1, test_x) # %>%
saveRDS(model1, file = "data-output/model1.RDS")
readRDS("data-output/model1.RDS")
model3 <- readRDS("data-output/model1.RDS")
history <- readRDS("data-output/history1.RDS")
plot(history)
save_model_tf(model1, "data-output/model1")
save_model_hdf5(model1, "data-output/model1.h5")
model1 <- load_model_hdf5("data-output/model1.h5")
history <- readRDS("data-output/history1.RDS")
plot(history)
pred <- predict(model1, test_x) # %>%
# k_argmax() %>% # take the highest probability value
# as.array() %>%
# as.factor()
pred <- predict(model1, test_x) # %>%
k_argmax() %>% # take the highest probability value
as.array() %>%
as.factor()
model1 <- load_model_hdf5("data-output/model1.h5")
history <- readRDS("data-output/history1.RDS")
plot(history)
pred <- predict(model1, test_x) # %>%
k_argmax() %>% # take the highest probability value
as.array() %>%
as.factor()
pred <- predict(model1, test_x) %>%
k_argmax() %>% # take the highest probability value
as.array() %>%
as.factor()
confusionMatrix(data=pred, reference = as.factor(data_test$label),)
# Change predictors to arrays
train_x <- array_reshape(data_train_x, dim=c(dim(data_train_x)[1],64,64,1))
val_x <- array_reshape(data_val_x, dim=c(dim(data_val_x)[1],64,64,1))
test_x <- array_reshape(data_test_x, dim=c(dim(data_test_x)[1],64,64,1))
# One-hot encoding target variable
train_y <- to_categorical(data_train_y)
val_y <- to_categorical(data_val_y)
test_y <- to_categorical(data_test_y)
input_dim <- ncol(train_x)
num_class <- n_distinct(data_train$label)
model2 <- keras_model_sequential() %>%
# Convolutional layer
layer_conv_2d(input_shape = c(64,64,1),
filters = 16,
kernel_size = c(3,3), # 3 x 3 filters
padding = "same",
activation = "relu") %>%
# Max pooling layer
layer_max_pooling_2d(pool_size = c(2,2)) %>%
# Flattening layer
layer_flatten() %>%
# Dense layer
layer_dense(units = 32,
activation = "relu") %>%
# output layer
layer_dense(units = num_class,
activation = "softmax",
name = "ouput")
model2 %>%
compile(loss = loss_categorical_crossentropy(),
optimizer = optimizer_adam(learning_rate = 0.01),
metrics = "accuracy")
history <- model2 %>%
fit(x = train_x,
y = train_y,
epoch = 15,
validation_data = list(val_x, val_y),
batch_size = 1000,
)
plot(history)
saveRDS(history, "data-output/history2.RDS")
save_model_hdf5(model2, "data-output/model2.h5")
pred <- predict(model2, test_x) %>%
k_argmax() %>%
as.array() %>%
as.factor()
confusionMatrix(data=pred, reference = as.factor(data_test$label))
model2 <- load_model_hdf5("data-output/model2.h5")
history <- readRDS("data-output/history.RDS")
model2 <- load_model_hdf5("data-output/model2.h5")
history <- readRDS("data-output/history.RDS")
model2 <- load_model_hdf5("data-output/model2.h5")
history <- readRDS("data-output/history.RDS")
model2 <- load_model_hdf5("data-output/model2.h5")
history <- readRDS("data-output/history2.RDS")
plot(history)
pred <- predict(model2, test_x) %>%
k_argmax() %>%
as.array() %>%
as.factor()
# clear-up the environment
rm(list = ls())
# chunk options
knitr::opts_chunk$set(
message = FALSE,
warning = FALSE,
fig.align = "center"
)
options(scipen = 9999)
library(keras)
library(dplyr)
library(caret)
chinese_mnist <- readRDS("data-input/chinese_mnist.RDS")
chinese_mnist <- chinese_mnist %>%
mutate(label = ifelse(label > 0, label-1, label))
sort(unique(chinese_mnist$label))
set.seed(100)
train_index <- createDataPartition(as.factor(chinese_mnist$label), p = 0.8, list = FALSE)
data_train <- chinese_mnist[ train_index,]
data_test  <- chinese_mnist[-train_index,]
set.seed(100)
test_index <- createDataPartition(as.factor(data_test$label), p = 0.5, list = FALSE)
data_val   <- data_test[ test_index,]
data_test  <- data_test[-test_index,]
data_train_x <- data_train %>%
select(-c(label,character,value)) %>% # take only the predictors
as.matrix()/255 # change the data type into matrix and do min-max scaling
data_train_y <- data_train$label # take only the labels
data_val_x <- data_val %>%
select(-c(label,character,value)) %>%
as.matrix()/255
data_val_y <- data_val$label
data_test_x <- data_test %>%
select(-c(label,character,value)) %>%
as.matrix()/255
data_test_y <- data_test$label
# Change predictors to arrays
train_x <- array_reshape(data_train_x, dim=c(dim(data_train_x)[1],64,64,1))
val_x <- array_reshape(data_val_x, dim=c(dim(data_val_x)[1],64,64,1))
test_x <- array_reshape(data_test_x, dim=c(dim(data_test_x)[1],64,64,1))
# One-hot encoding target variable
train_y <- to_categorical(data_train_y)
val_y <- to_categorical(data_val_y)
test_y <- to_categorical(data_test_y)
input_dim <- ncol(train_x)
num_class <- n_distinct(data_train$label)
model2 <- keras_model_sequential() %>%
# Convolutional layer
layer_conv_2d(input_shape = c(64,64,1),
filters = 16,
kernel_size = c(3,3), # 3 x 3 filters
activation = "relu") %>%
# Max pooling layer
layer_max_pooling_2d(pool_size = c(2,2)) %>%
# Flattening layer
layer_flatten() %>%
# Dense layer
layer_dense(units = 32,
activation = "relu") %>%
# output layer
layer_dense(units = num_class,
activation = "softmax",
name = "ouput")
model2 %>%
compile(loss = loss_categorical_crossentropy(),
optimizer = optimizer_adam(learning_rate = 0.01),
metrics = "accuracy")
history <- model2 %>%
fit(x = train_x,
y = train_y,
epoch = 15,
validation_data = list(val_x, val_y),
batch_size = 1000,
)
plot(history)
pred <- predict(model2, test_x) %>%
k_argmax() %>%
as.array() %>%
as.factor()
confusionMatrix(data=pred, reference = as.factor(data_test$label))
save_model_hdf5(model2, "data-output/model2.h5")
saveRDS(history, "data-output/history2.RDS")
model2 <- load_model_hdf5("data-output/model2.h5")
history <- readRDS("data-output/history2.RDS")
plot(history)
pred <- predict(model2, test_x) %>%
k_argmax() %>%
as.array() %>%
as.factor()
confusionMatrix(data=pred, reference = as.factor(data_test$label))
# Change predictors to arrays
train_x <- array_reshape(data_train_x, dim=dim(data_train_x))
val_x <- array_reshape(data_val_x, dim=dim(data_val_x))
test_x <- array_reshape(data_test_x, dim=dim(data_test_x))
# One-hot encoding target variable
train_y <- to_categorical(data_train_y)
val_y <- to_categorical(data_val_y)
test_y <- to_categorical(data_test_y)
model1 <- load_model_hdf5("data-output/model1.h5")
history <- readRDS("data-output/history1.RDS")
plot(history)
pred <- predict(model1, test_x) %>%
k_argmax() %>% # take the highest probability value
as.array() %>%
as.factor()
confusionMatrix(data=pred, reference = as.factor(data_test$label),)
